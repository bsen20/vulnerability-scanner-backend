import requests
import sys
from urllib.parse import urlparse

# the domain to scan for subdomains
# domain = "google.com"
# read all subdomains
file = open("subdomains.txt")
# read all content
content = file.read()
# split by new lines
subdomains = content.splitlines()
# a list of discovered subdomains
def get_domain_from_url (url):
    
    domain = urlparse(url).netloc
    cnt = 0
    for i in domain:
        if i == '.':
            cnt = cnt+1
    if cnt > 1:
        idx = domain.find('.')
        domain = domain[idx+1:]        
        
    print(domain)
    return domain
    
def find_sub_domains(domain):
    for subdomain in subdomains:
        # construct the url
        url = f"http://{subdomain}.{domain}"
        try:
            # if this raises an ERROR, that means the subdomain does not exist
            requests.get(url)
        except requests.ConnectionError:
            # if the subdomain does not exist, just pass, print nothing
            pass
        else:
            print("[+] Discovered subdomain:", url)
            # append the discovered subdomain to our list

if __name__ == "__main__":
    url = sys.argv[1] 
    # url = "https://www.google.com"
    find_sub_domains(get_domain_from_url(url))
          
